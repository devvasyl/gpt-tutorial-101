# 【应用】BloombergGPT：金融领域的大语言模型

2023 年 3 月 31 日，金融领域领头羊之一的彭博社（Bloomberg）发布了专门为金融领域打造的大型语言模型(LLM)——BloombergGPT，它对于 GPT 的应用也许不是最佳答案，但是交卷最快的答案，也许我们可以从这份答案里略窥一二。

> 注：因为由于主题更偏向于 GPT/LLM 的行业应用，但当前的介绍也是基于论文的内容，所以被同样放在了【麻瓜读论文】的系列里面

从论文来看，BloombergGPT 并非是一个金融版的 ChatGPT ，至少从产品形态上来说并不是的。BloombergGPT 主要是用于协助彭博改进现有的金融 NLP 任务，如情感分析、命名实体识别、新闻分类和问答等。

从 LLM 相关的技术上来说，这篇论文事件了几个可以借鉴的思路：

- 垂直领域语言模型：BloombergGPT 实践了通用+垂直的混合训练方法，根据论文的说法，同样的参数规模下，效果在特定领域是要比通用模型更好的；
- 更智能的 Tokenizer：使用了 Unigram 模型取代 greedymerge-basedsub-word 模型，实现更智能的 tokenizer；
- 成本更低的模型构建方法：开源模型 BLOOM + 自有高质量数据，中等规模团队是可以做出一个优质的特定领域大模型的；

论文作者提到了三个主要的场景应用：

- 生成彭博终端的查询语言；
- 新闻标题的建议；
- 金融问答；

虽然目前看起来应用场景并不算丰富，对于无法负担 Bloomberg 终端的普通投资者来说也无法触及，但从当前这篇论文看起来，Bloomberg 更多是在探索如何构建专用场景下的底层大模型，而上层的产品形态关注较少，我们可以持续期待未来模型更成熟后金融领域的新形态。

## 论文信息

论文链接：

**BloombergGPT: A Large Language Model for Finance**

[https://arxiv.org/abs/2303.17564](https://arxiv.org/abs/2303.17564)

论文基本信息：

- 标题：BloombergGPT：应用在金融的大语言模型
- 作者：[Shijie Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Ozan Irsoy](https://arxiv.org/search/cs?searchtype=author&query=Irsoy%2C+O), [Steven Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+S), [Vadim Dabravolski](https://arxiv.org/search/cs?searchtype=author&query=Dabravolski%2C+V), [Mark Dredze](https://arxiv.org/search/cs?searchtype=author&query=Dredze%2C+M), [Sebastian Gehrmann](https://arxiv.org/search/cs?searchtype=author&query=Gehrmann%2C+S), [Prabhanjan Kambadur](https://arxiv.org/search/cs?searchtype=author&query=Kambadur%2C+P), [David Rosenberg](https://arxiv.org/search/cs?searchtype=author&query=Rosenberg%2C+D), [Gideon Mann](https://arxiv.org/search/cs?searchtype=author&query=Mann%2C+G)
- 摘要：
    
    > 在金融科技领域中，自然语言处理的应用范围广泛而复杂，包括情感分析、命名实体识别和问答等。大型语言模型（LLM）已被证明在各种任务上都非常有效，但是在金融领域专门针对性开发的LLM在文献中并未报道。在这项工作中，我们提出了BloombergGPT，这是一个500亿参数的语言模型，经过广泛的金融数据训练而成。我们基于彭博社广泛的数据源构建了一个3630亿 token数据集，或许是迄今为止最大的领域特定数据集，还加入了3450亿来自通用数据集的 token。我们在标准LLM基准测试、开放金融基准测试和一系列最准确反映我们预期使用的内部基准测试上验证了BloombergGPT。我们的混合数据集训练导致了一个模型，在金融任务上比现有模型表现出显著的优势，而不损害在通用LLM基准测试上的性能。此外，我们解释了我们的建模选择、训练过程和评估方法。下一步，我们计划发布训练日志（编年史），详细记录我们训练BloombergGPT的经验。
    > 

## 论文正文中译

### 摘要

略

### 1. Introduction

2020年GPT-3的发布（Brown et al., 2020）展示了训练非常大的自回归语言模型（LLMs）的强大优势。GPT-3拥有1750亿个参数，比先前的GPT-2模型增加了100倍，并在现在流行的LLM任务范围内表现出色，包括阅读理解，开放问题回答和代码生成。这种性能已经在其他几个模型中得到了复制（Chowdhery et al., 2022; Scao et al., 2022; Zhang et al., 2022a）。此外，证据表明，大型模型表现出新兴行为;增长使它们获得了在较小模型中不存在的能力（Wei et al., 2022a）。新兴行为的一个显着例子是通过少量提示执行任务的能力，其中模型只需从几个示例中学习任务。随着语言模型的大小增加，这种能力明显优于随机性。总的来说，少量提示显着扩展了模型支持的任务范围，并降低了寻求新语言任务自动化的用户的门槛。

在GPT-3之后，模型的规模增长到了2800亿（Gopher，Rae等人，2021），5400亿（PaLM，Chowdhery等人，2022）和1万亿个参数（Megatron，Korthikanti等人，2022）。工作还探讨了实现高性能LLM的其他重要方面，例如不同的训练目标（Tay等人，2022b），多语言模型（Scao等人，2022），更高效和更小的模型（Black等人，2022）以及找到数据和参数高效训练大小（Hoffmann等人，2022）。

这些努力几乎完全集中在一般LLMs上，这些模型是在覆盖广泛的主题和领域的数据集上进行训练的。虽然这些包括一些专业领域的数据集（例如代码（Chen等人，2021a）或生物医学文章Gao等人（2021）），但重点是建立具有广泛能力的LLMs。最近的努力仅使用特定领域的数据训练模型，这些模型虽然小得多，但在这些领域内击败了通用LLMs，例如科学Taylor等人（2022）和医学Bolton等人（2023）;Luo等人（2022）;Lehman等人（2023）。这些发现激励进一步开发专注于特定领域的模型。

金融技术（FinTech）是一个庞大而不断增长的领域，自然语言处理技术在其中扮演着越来越重要的角色（Xing等人，2018; Fisher等人，2016; Dredze等人，2016）。金融NLP任务Shah等人（2022）包括情感分析Araci（2019），命名实体识别Salinas Alvarado等人（2015），新闻分类Sinha和Khandait（2020）和问答Chen等人（2021b，2022）。尽管任务范围与一般NLP基准中的任务范围相似，但金融领域的复杂性和术语需要专门的系统。由于生成LLMs在一般情况下很有吸引力-少量学习，文本生成，会话系统等-因此拥有专注于金融领域的LLM将是有价值的。虽然已经为金融领域调整了掩码语言模型Araci（2019），但尚未为此领域调整或评估LLM上的任务。

#### 1.1 BloombergGPT

我们训练BloombergGPT，一个支持金融行业内广泛任务的500亿参数语言模型。我们采取混合方法，而不是构建通用LLM或仅基于特定领域数据的小型LLM。一般模型涵盖许多领域，能够在各种任务上高水平表现，并且在训练时消除了专业化的需要。然而，现有领域特定模型的结果表明，通用模型无法取代它们。在Bloomberg，我们支持非常大而多样化的任务集，这些任务可以通过通用模型很好地完成，但我们大部分应用都在金融领域，需要专门的模型。出于这个原因，我们着手建立一个模型，该模型在金融基准测试中取得最佳结果，同时在通用LLM基准测试中保持竞争性表现。

我们通过利用Bloomberg现有的数据创建，收集和筛选资源，构建迄今为止最大的特定领域数据集。作为一家主要的金融数据公司，我们的数据分析师已经收集和筛选了40年的金融语言文件。我们拥有涵盖各种主题的大量金融数据，并仔细跟踪数据来源和使用权。我们将这些数据加入公共数据集，创建一个包含超过7000亿个 token的大型训练语料库。使用这个训练语料库的一部分，我们训练了一个基于Hoffmann等人（2022）和Le Scao等人（2022）的指南设计的BLOOM风格的500亿参数模型。我们在标准LLM基准测试，开放金融基准测试和一套最能反映我们预期使用情况的Bloomberg内部基准测试上验证了该模型。我们的结果表明，我们的混合训练方法导致一个模型，在特定领域的金融任务上远远优于现有模型，同时在通用NLP基准测试上保持相当或更好的表现。

#### 1.2更广泛的贡献

除了构建金融数据的LLM之外，我们的目标是为更广泛的研究社区做出贡献。具体而言，我们在本文中记录的经验提供了证据，进一步发展了社区对文献中几个未解决问题的理解。

***特定领域的LLMs。*** 几乎所有现有的特定领域LLMs都是仅在特定领域的数据源上训练的（Luo等人，2022; Bolton等人，2023; Taylor等人，2022），或者将非常大的通用模型适应于特定领域的任务（Singhal等人，2022; Lewkowycz等人，2022）。我们的替代方法-在特定领域和通用数据源上训练LLM-迄今为止尚未研究过。由此产生的模型在特定领域任务上表现非常出色，但在通用目的基准测试上也保持着强大的性能。

***训练数据。*** 几乎所有语言模型在很大程度上依赖于从网站获取的数据，例如C4（Raffel等人，2020）和The Pile（Gao等人，2021）（其中包括OpenWebText2）。此数据可以在使用之前以各种方式进行清理或子集化（Touvron等人，2023）；Rae等人（2020）；Scao等人（2022）；Jernite等人（2022），但数据重复（Carlini等人，2020）和有毒语言的问题仍然存在（Welbl等人，2021）。我们的训练数据对LLM训练来说是不寻常的，因为它包括大量来自可靠来源的策划和准备好的数据。

***评估。***  Gehrmann等人（2022）; Goyal等人（2022）仍然存在LLM评估的挑战和发展问题，新的基准试图标准化跨模型的评估（Liang等人，2022; Srivastava等人，2022）。然而，对于特定领域的任务，评估与实际用例之间存在不匹配。评估是建立在可用数据集上的，而不一定是模型在实践中的使用方式。我们提供了公共金融NLP基准测试（Shah等人，2022; Chen等人，2021b）以及一些内部Bloomberg任务的结果，这些任务更符合我们预期的用例，并直接评估我们的模型执行感兴趣的任务的能力。

***模型大小。*** 早期的LLM对200-4000亿个 token的语料库进行了单次训练（Brown等人，2020），Hoffmann等人（2022）认为模型未经训练，而是专注于使用更多数据训练较小的模型，这是Touvron等人（2023）最近采用的策略。我们选择了一个受Hoffmann等人（2022）激励的模型大小，并在我们超过7000亿个 token的语料库中对5690亿个 token进行了50亿个参数的模型训练，以产生与更大的模型竞争的模型。

Tokenizer***。*** 在组装训练数据之后， token化的关键步骤将文本转换为适合语言模型的格式。这一步骤的重要性经常被忽视Mielke等人（2021），许多较老的LLM使用相同的 token器和词汇表，这意味着我们很少有支持其他 token器的证据。我们采用不同的方法，使用Unigram模型而不是贪婪合并子单词 token器，因为它保存概率，允许在推理时进行更智能的 token化（Kudo，2018）。

***模型构建挑战。***  GPT-3和随后的模型是大型团队的工作，需要大量计算。尝试复制这些结果的初始工作，例如OPT Zhang等人（2022a），没有达到原始模型的性能。随着每个随后的模型的发布，社区的理解，经验和软件工具都在增加。在开发BloombergGPT时，我们从BLOOM努力Scao等人（2022）开发的现有代码中受益，表明中等规模的团队可以在特定领域的数据上产生有竞争力的模型。我们详细描述了训练BloombergGPT的经验，以支持未来的训练工作，并解决上述每个主题。

### 2. 数据集

为了训练BloombergGPT，我们构建了“FinPile”，这是一个包括新闻、申报、新闻稿、网络爬取的金融文件和社交媒体，从Bloomberg档案中抽取的包括过去二十年的文档的综合数据集。我们使用广泛用于训练LLMs的公共数据增强FinPile。结果是一个训练语料库，大约一半是特定领域的文本，一半是通用文本。有关完整训练集的详细信息，请参见表1。为了提高数据质量，我们根据Lee等人（2022a）对每个数据集（The Pile、C4、Wikipedia、FinPile）进行了去重处理；作为副作用，表1中报告的统计数据可能与其他论文中报告的数据不同。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled.png)

Table 1: Breakdown of the full training set used to train BloombergGPT. The statistics
provided are the average number of characters per document (“C/D”), the average
number of characters per token (“C/T”), and the percentage of the overall tokens
(“T%”). Units for each column are denoted in the header.

#### 2.1 金融数据集（363Btoken - 训练的54.2％）

Bloomberg Terminal为过去四十年提供了全面的、多样化的结构化和非结构化金融数据和分析。为了服务这一使命，Bloomberg分析师策划了一组金融文件，这些文件要么是内部创建的，要么是从外部来源获取的。我们利用这个广泛的策划和维护的文档集合创建了FinPile，其中包括公司文件、财经新闻和其他与金融市场相关的数据。

FinPile中包括的一些文件，如公司文件，对于普通公众是可获取的，尽管收集这些文件并为LLM培训进行预处理是一个不容易的任务。其他文件，如（部分）彭博新闻，必须购买。其余的文件是私有的，可通过彭博终端等其他来源获得。最后，我们清理这些数据以去除 token、特殊格式和模板。

请注意，FinPile中的每个文档都有时间戳，日期范围从2007年3月1日到2022年7月31日；文档的质量和数量在此时间范围内增加。虽然我们在这项工作中不使用日期信息，但我们计划将来使用它，例如评估模型对不同时间段学习的内容。虽然我们不能发布FinPile，但我们在一个大型、经过精心策划和清理的领域特定数据集上的训练经验可能为社区提供有关特定金融LLM建模的优势和挑战的有用见解。我们在表2中提供了FinPile的拆分和分析，以下是所包含数据类型的简要描述。

##### 2.1.1 网络（298B token - 训练的42.01％）

Bloomberg通过识别包含金融相关信息的网站来收集网络内容。虽然此类别占FinPile的大部分，但其分类粗略，主要根据网域位置进行分类。在这些位置特定的来源中，例如“美国”（总共的15.95％）、“亚太地区”（总共的4.72％）和“英国”（总共的1.98％），文档类型高度多样化，这是Web爬行的预期结果。虽然Web来源在现有的公共LLM培训数据集中很常见，但Bloomberg的Web爬行集中在具有金融相关信息的高质量网站上，而不是普通Web爬行。

##### 2.1.2 新闻（38B token - 训练的5.31％）
新闻类别包括除彭博新闻记者撰写的新闻文章外的所有新闻来源。总体而言，FinPile中有数百种英语新闻来源，包括“彭博转录”（总共的0.41％），它们是彭博电视新闻的转录。一般而言，这个数据集中的内容来自于对金融社区有关的有信誉的新闻来源，以保持事实性和降低偏见。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%201.png)

Table 2: The number of tokens (in millions) contained within documents in FinPile, organized by year (rows) and type (column). Units are millions of tokens.

##### 2.1.3 文件（14B token - 训练的2.04％）

公司文件是由（公共）公司编制的财务报表，并提供给公众。在一些国家，比如美国，公共公司被要求定期准备和提交他们的财务报表，例如10-K年度报告和10-Q季度报告。在我们的数据集中，大部分文件来自于EDGAR，这是SEC的在线数据库（总计1.90％）。文件通常是长篇的PDF文档，包含着密集的财务信息，其中包含的表格和图表在Bloomberg中被处理和规范化。文件与通常用于训练LLMs的文档类型有很大的不同，但对于金融决策来说包含着至关重要的信息。

##### 2.1.4 新闻发布（9B个 token-训练的1.21％）
新闻发布类别包含通常由财务相关的公司发布的新闻稿。与文件一起，新闻发布代表了公司大部分的公共沟通。然而，与文件不同，新闻发布与内容和风格类似于新闻报道。

##### 2.1.5 Bloomberg（5B个 token-训练的0.70％）
该类别包括归属于Bloomberg的新闻和其他文件，例如意见和分析。最大的来源是“Bloomberg News”（总计0.44％）和“Bloomberg First Word”（总计0.13％），这是Bloomberg撰写的实时新闻电线。虽然Bloomberg新闻涵盖了各种各样的主题，但它通常侧重于与金融社区相关的内容。该数据集包含不同长度的文档。

#### 2.2 公共数据集 (345B个Token - 占训练数据的48.73%)

我们在训练语料库中使用了三个广为人知和可用的公共数据集。

##### 2.2.1 The Pile (184B个 token - 占训练数据的25.9%)

The Pile (Gao et al., 2021)是GPT-Neo(Black et al., 2021)，GPTJ(Wang and Komatsuzaki, 2021)和GPT-NeoX(20B)(Black et al., 2022)使用的数据集。我们将The Pile包含在我们的训练数据中有以下原因。首先，它已经成功地用于训练LLM。其次，它已经经过了重要的数据清理和预处理。第三，它包括多个域，我们认为这样多样化的数据将有助于推广到新的领域，并可能支持在金融数据上进行培训。例如，FreeLaw和GitHub这样的领域对于在彭博公司从事法律文件和软件开发的团队非常有用。The Pile的创建者故意选择包括重复内容，其中重复因子与内容的质量成比例。但是，当我们去重每个数据集时，The Pile的大小显着减小。此外，请注意我们的分词器(§2.3)是在The Pile上进行训练的。

##### 2.2.2 C4 (138B个 token - 占训练数据的19.48%)

巨大的清理抓取语料库(C4)是用于训练LLM的常见数据集，并且是用于支持T5(Raffel et al., 2020)的。尽管它与Pile-CC重叠，但C4的清理和处理方式不同；因此，我们认为除了复制的文档之外，包括C4还可以增加价值。我们发现C4包含高质量的自然语言文档，因为它的清理层次，尽管其他人指出，来自专利的数据比例很高，分布在网络域中不寻常(Dodge et al.，2021)。

##### 2.2.3 维基百科 (24B个 token - 占训练数据的3.35%)

由于The Pile和C4都包括过时的维基百科副本，因此将最新的维基百科页面包含在内可能是有益的。因此，我们包含了一个来自2022年7月1日的英文维基百科转储。这个数据集的 token化效率相当低(每个 token3.06个字符)，表明其中有一个高于平均水平的 token量，这表明进一步的清理可能有利于未来的模型训练。

#### 2.3 分词

我们选择Unigram分词器（Kudo，2018）而不是贪婪的基于合并的子词分词器，如Byte Pair Encoding（BPE）（Sennrich等人，2016）或Wordpiece（Schuster和Nakajima，2012；Wu等人，2016），基于Kudo和Richardson（2018）以及Bostrom和Durrett（2020）的有前途的结果。沿袭GPT-2 Radford等人（2019）的做法，我们将数据视为字节序列而不是Unicode字符，并将每个256个字节作为 token。在预分词步骤中，通过贪婪地匹配以下正则表达式将输入字节序列分成块：[A-Za-z]+ | [0-9] | [^A-Za-z0-9] +。这遵循了GPT-2的做法，防止多个字符类出现在单个 token中。然而，我们在字母块中包括空格，这允许学习多字词 token，增加信息密度并减少上下文长度。预分词遵循了PaLM Chowdhery等人（2022）的方法，将每个数字放在自己的块中，希望这将带来更好的数字处理。我们在The Pile Gao等人（2021）上训练我们的分词器，因为它从各种领域中汲取，包括代码和学术论文，比例适合我们的用例。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%202.png)

Table 3: Number of tokens in each training dataset with BLOOM, NeoX, OPT (GPT2),
and BloombergGPT tokenizers. All token counts are in billions (B). Note that
an older version of FinPile was used for this count, so token numbers will not
match earlier tables.

***并行分词器训练。***  Unigram分词器实现过于低效，无法一次处理整个Pile数据集，因此我们采用分割和合并的方法。我们将Pile中的22个领域分成256个大小大致相等的块。然后我们对每个22 × 256（总计= 5,632）块训练一个词汇量为65,536（216）的Unigram分词器。我们通过首先合并每个领域的256个分词器，然后组合22个分词器来层次化地合并单个分词器，以获得最终的分词器。Unigram分词器相当于 token上的概率分布（即单字语言模型），我们通过取相应 token的概率的加权平均值来合并分词器，权重由用于训练分词器的数据的相对大小（以字节为单位）确定。结果是一个具有700万个 token的分词器。为减少词汇量到217个 token，我们放弃了概率最小的 token并重新归一化。为确保我们不需要一个词汇表外的 token，我们还添加了不在Pile中出现的256个字节中的36个字节，以及一个<|endoftext|> token。

在选择词汇量时有多种考虑因素。LLM使用大词汇量的一个优点是可以将更多信息放入上下文窗口。另一方面，使用更大的词汇量会带来一些开销：需要更多的模型参数来进行 token嵌入。我们根据词汇表从25,000到550,000进行的实验选择了217个 token的词汇量。对于每个词汇量，我们对C4数据集进行分词，并计算数据集的总大小（以字节为单位），其中每个 token使用log2（词汇量大小）位表示。我们的启发式方法是选择导致C4最小编码表示的词汇量大小。这给我们一个125,000的词汇量大小，然后我们将其四舍五入到最接近的2的幂（217或131,072个 token）。相对于标准词汇量大小（约50,000个 token），我们的分词器要大得多。有关分词效率的分析，请参见表3。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%203.png)

### 3. 模型

#### 3.1 架构

我们的模型是基于BLOOM（Scao等人，2022）的仅解码器因果语言模型。
我们提供了架构概述，完整细节在附录A中。
该模型包含70层变压器解码器块，定义如下：

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%204.png)

其中SA是多头自注意力，LN是层归一化，FFN是一个带有1个隐藏层的前馈网络。在FFN中，非线性函数是GELU（Hendrycks和Gimpel，2016）。ALiBi位置编码通过添加偏置应用于变换器网络的自注意组件（Le Scao等人，2022）。输入 token嵌入与最终softmax之前的线性映射相关联。按照Le Scao等人（2022）和Dettmers等人（2022）的方法，模型在 token嵌入后具有额外的层归一化，形式上为：

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%205.png)

其中h0是初始的 token嵌入，LNem是新的嵌入层规范化组件。请注意，第二项包括两个连续的层规范化。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%206.png)

#### 3.2 模型缩放

大小。我们的模型大小基于Chinchilla比例定律（Hoffmann等人，2022），特别是它们的第一种方法和第二种方法。我们从40GB A100 GPU的总计算预算1.3M GPU小时开始。由于我们采用激活检查点技术来减少内存占用，这会导致每次迭代额外的0.33x TFLOPs成本，因为需要重复前向传递。为了考虑这种额外成本，我们插入0.75×1.3M而不是全部金额到Chinchilla方程中。

从Hoffmann等人（2022）中，我们使用表3的数据来进行第一种方法的计算，使用表A3进行第二种方法的计算，并对它们的对数版本拟合回归线。这给出了我们以下计算结果：

第一种方法参数=exp10(log10(FLOPs)·0.498−1.004)=52.993B
 token=exp10(log10(FLOPs)·0.502+0.229)=1111.112B
第二种方法参数=exp10(log10(FLOPs)·0.490−0.839)=49.753B
 token=exp10(log10(FL OPs) · 0.510 + 0.062) = 1175.766B

这些计算表明，我们的~700B token数据集对于我们的计算预算来说太小了，无法实现“Chinchilla最优”配置（假设只经过一次数据）。虽然我们可以增加通用训练数据的数量，但我们受制于我们可以使用的特定领域训练数据的数量。FinPile已经是最大的特定领域训练集之一，我们不希望它代表我们的总训练量少于一半。

由于我们的数据受限，我们选择最大的模型，同时确保我们可以在所有 token上进行训练，并仍然将总计算预算的约30％留作缓冲区，以应对意外故障、重试和重新启动。这导致我们选择50B参数模型，这也大致是我们计算预算的Chinchilla最优大小。图1提供了比例定律的摘要以及BloombergGPT与其他模型的比较。

形状。为了确定如何将50B参数分配给不同的模型组件（即我们模型的“形状”），我们遵循Levine等人（2020）的建议，在总自注意力层数L的情况下，最佳隐藏维度D是通过以下公式获得的：

D = exp(5.039) exp(0.0555 · L)

我们在整数值的范围内扫描L，并选择产生总共约50B参数的（L，D）组合。这导致我们选择L = 70和D = 7510作为目标形状参数。但是，我们还希望遵循隐藏维度可以均匀地被注意力头数整除的传统，商数给出注意力头维度。此外，我们希望维度是8的倍数，以在Tensor Core操作中实现更高的性能（NVIDIA，2023）。我们选择40个头，每个头具有192个维度，从而得到总隐藏维度为D = 7680，总共有50.6B参数。表4提供了BloombergGPT中使用的超参数的摘要。

#### 3.3 培训配置

***培训。***  BloombergGPT是一个基于PyTorch的模型，采用标准的从左到右的因果语言建模目标进行训练。按照Brown等人（2020）的建议，我们希望我们所有的训练序列长度完全相同，我们的情况下为2048个 token，以最大化GPU利用率。为了实现这一点，我们使用一个` <|endoftext|> ` token 作为文件分割符。

然后，我们将这个 token序列分成2048个 token的块。请注意，使用这种方法，每个训练序列可能包含来自不同领域的多个文档。还请注意，因为我们使用了ALiBi位置编码，BloombergGPT可以应用于长度超过2048的序列。为了优化效率，训练序列被分成批次，如下面所述。

***优化。***  我们使用AdamW优化器（Loshchilov和Hutter，2019年）。我们将β1设置为0.9，将β2设置为0.95，将权重衰减设置为0.1。按照Brown等人（2020年）的方法，我们将最大学习率设置为6e-5，并使用余弦衰减学习率调度程序进行线性预热。我们在前1800步中预热学习率。根据Hoffmann等人（2022年）的方法，最终学习率是最大学习率的0.1倍，即6e-6。我们还采用批量大小预热（Brown等人，2020年）：在前7200步中，我们使用1024个批量大小（2.1M token），然后切换到2048个批量大小（4.2M token）进行训练的其余部分。我们在初始运行中将所有层的丢失率设置为0.0，尽管稍后我们会添加丢失率，如§4中所述。模型参数随机初始化为从具有零平均值和标准偏差p1/（3D）=0.006588的正态分布中采样的样本（Smith等人，2022年）。按照Megatron-LM（Shoeybi等人，2019年）的方法，我们通过1/√2L来重新缩放MLP中第二层和注意力输出层的标准偏差。我们使用查询键层缩放技术（Shoeybi等人，2019年），该技术旨在提高FP16混合精度训练的数值稳定性，但也可能有助于BF16。

***培训不稳定性。***  LLMs优化需要在极其复杂的非凸损失表面上运行凸优化算法。以前的工作报告了在训练LLMs时出现各种不稳定性。例如，Chowdhery等人（2022年）发现，尽管启用了梯度剪裁，PaLM的损失大约会上涨20次。他们通过从距离尖峰约100步的检查点重新启动训练，然后跳过200-500个数据批次来缓解这些问题。他们假设尖峰是由特定数据批次与特定模型参数状态的组合引起的。同样，在OPT训练期间，Zhang等人（2022a）注意到梯度和激活范数的尖峰或训练困惑的发散。在这些行为之后，他们降低了学习率，稳定了这些范数，并允许继续训练。有趣的是，Scao等人（2022年）报告了仅一个损失尖峰，模型自行恢复。

***硬件堆栈。***  我们使用AWS提供的Amazon SageMaker服务来训练和评估BloombergGPT。我们使用训练时可用的最新版本，并在总共64个p4d.24xlarge实例上进行训练。每个p4d.24xlarge实例都配备有8个NVIDIA 40GB A100 GPU，并具有NVIDIA NVSwitch节点内连接（600 GB/s）和NVIDIA GPUDirect使用AWS Elastic Fabric Adapter（EFA）节点间连接（400 Gb/s）。这总共有512个40GB A100 GPU。为了快速访问数据，我们使用Amazon FSX for Lustre，它支持每个TiB存储单元高达1000 MB/s的读取和写入吞吐量。

#### 3.4 大规模优化

为了训练具有比云实例上可用GPU内存更大的内存占用的BloombergGPT，我们依赖于ZeRO优化的第3阶段(Rajbhandari等人，2020)。我们利用AWS的专有SageMaker模型并行性(SMP)库，该库能自动将大型模型分发到多个GPU设备和实例上(Karakus等人，2021)。在尝试了各种技术后，我们平均达到了102 TFLOPs，每个训练步骤需要32.5秒。我们发现以下设置在训练中表现最佳。

***ZeRO优化(第3阶段)。*** ZeRO将训练状态(模型参数、梯度和优化器状态)分布在一组GPU上。我们将模型分布在128个GPU上，并在训练期间拥有4个模型副本。

***MiCS。*** Zhang等人(2022b)减少了云训练集群的训练通信开销和内存需求。MiCS包括分层通信、2跳梯度更新、比例感知模型分区等功能。

***激活检查点。*** Chen等人(2016)通过在后向传递期间进行额外计算，最小化了训练内存消耗，从而消除了激活。当一个层启用激活检查点时，只有层输入和输出在前向传递后保留在内存中，而任何中间张量都从内存中丢弃。在后向传递期间，这些中间张量可能会被重新计算。我们将激活检查点应用到每个变形器层。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%207.png)

***混合精度训练。*** 为了减少内存需求，前向和后向传递使用BF16进行，而参数以完整精度(FP32)存储和更新。ALiBi矩阵以完整精度计算并存储在BF16中。我们还使用FP32在Attention块中计算融合softmax并将其结果存储在BF16中。最后，损失函数中的softmax计算以FP32计算。

***融合内核。*** 另一种优化可能性是将几个操作的组合合并为单个GPU操作。这既可以通过避免在计算图中存储中间结果来减少峰值内存使用，也可以帮助提高速度。与Megatron-LM Shoeybi等人(2019)类似，我们在自我关注模块中使用了一个掩码-因果-softmax融合内核。在实践中，我们观察到速度提高了4-5 TFLOPs，并且避免了由于余下的配置而导致的内存不足错误。

### 4. Training Run

BloombergGPT的培训过程基于模型训练进展做出决策。我们分享了这个过程的一些亮点。图2展示了训练和验证集的学习曲线。实线表示（平滑后的）训练损失，虚线表示验证集上的损失。线条颜色的变化表示优化超参数配置的变化，要么按计划，要么响应于验证损失的增加或停滞。该图显示了成功模型训练运行的路径。为了呈现清晰的图形，该图未显示使用不同模型配置的其他尝试、回滚后覆盖的部分运行或未在最终模型中使用的其他训练策略。

我们每五个步骤测量一次当前批次的训练损失。原始值变化很大，绘制时会产生大的抖动。该图通过展示运行平均值yt=
Pt
i=0 xi·(1−α)
(t−i)
Pt
i=0(1−α)
(t−i)（其中α=0.001）平滑训练损失。由于验证损失是每300步在整个验证集上测量的，因此不需要平滑。

我们总共训练了139,200步（约53天），并在完成我们的训练数据的大约80％的一个时期（共计569B个 token，共计709B个可用 token）后结束了模型训练。我们提前结束训练，因为我们的开发集上的损失不再改善，尽管可能会进行更长时间的训练以获得进一步的改进。

我们开始运行时，热身批量大小为1,024，持续7,200步，然后切换到正常批量大小2,048（颜色从黑色变为蓝色）。批量大小的变化在第7,200步的验证损失中表现为可见的曲率变化。在大部分其余的训练中，训练和验证损失都稳定下降。在后期阶段，即在第115,500步之后，我们观察到验证损失停滞或增加，需要进行干预。然后，我们按顺序应用以下纠正修改：

- 第115,500步（蓝色到橙色）：将学习率缩小为三分之二
- 第129,900步（橙色到绿色）：将学习率减半，并添加0.1概率的dropout
- 第137,100步（绿色到红色）：再次将学习率减半

我们在第146,000步结束运行，因为验证损失没有明显的进展。我们根据验证损失和下游评估选择了第139,200步的检查点作为最终模型。

### 5. 评估

我们评估了BloombergGPT在两个广泛的任务类别上的表现：金融特定和通用目的。金融特定任务帮助我们测试我们的假设，即在高质量的金融特定数据上进行培训将在金融任务上产生更好的结果。通用目的任务调查了我们的模型的性能是否直接可比于先前发布的结果。对于金融任务，我们收集了包括一系列自然语言处理任务的公开金融数据集。然后，为了直接测试BloombergGPT在Bloomberg感兴趣的任务上的能力，我们还包括了来自Bloomberg内部高质量评估集的情感分析和命名实体识别任务。对于通用目的任务，我们从多个现有基准测试中提取结果，并将其分为以下类别：BIG-bench Hard、知识评估、阅读理解和语言任务。每种类型的任务数量和组的定义见表5。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%208.png)

我们将BloombergGPT与第7节中描述的三个最接近的模型进行比较，比较基于模型大小、训练数据类型、整体性能以及最重要的是访问权限。表6提供了模型大小和计算的概述。

1. GPT-NeoX（Black等人，2022年）：根据Liang等人（2022年）的说法，该模型是50B参数以下的最佳性能可用模型。
2. OPT66B（Zhang等人，2022a）：我们选择与OPT66B进行比较，因为我们的模型大小和结构大致相同，尽管我们的模型较小。
3. BLOOM176B（Scao等人，2022年）：虽然这个模型比BloombergGPT大得多，但我们使用相同的模型架构和软件堆栈。我们注意到BLOOM176B是多语言的，因此虽然它更大，但它也是训练了更多语言的数据。

这三个模型都使用了我们在训练语料库中使用的一些通用数据集。我们还额外报告了来自原始GPT-3（Brown等人，2020年）的结果，只要它们在外部可用。

为了确保相同的评估设置，我们更喜欢自己运行模型，并将任何已在其他地方报告并且不是我们运行的结果放入一个单独的组中。为了公平地比较模型，我们避免对提示和其他技术进行任何调整，这可能会导致某些模型的结果改善，但不是所有模型都是这样。因此，每个任务都通过“标准”提示进行测试（如表7所示），即不对底层模型进行任何参数更改，不含任务描述以及不含思维链提示（Wei等人，2022b）。向模型提供的少量示例数量取决于任务，我们在各自的部分中包括这些细节。对于每组结果，我们进一步提供了与Liang等人（2022年）类似的胜率，表示在所有模型对之间的单个任务的并排比较中的“获胜”比例，我们已经运行了自己的评估。

#### 5.1 少样本方法论

对于给定一组候选项的任务，我们执行基于似然概率的分类，遵循Brown等人（2020年）的方法。我们考虑三种分类方法：常规、校准和归一化。形式上，

- 常规：arg maxα p(α|s)
- 校准：arg maxα p(α|s)/p(α|“Answer:”)
- 归一化：arg maxα p(α|s)/len(α)

其中α是候选项，s是上下文，len衡量子单词 token的数量。我们报告每个模型和任务的最佳方法的性能。对于其他任务，我们通过贪婪解码进行生成。

我们使用官方的分割，并在可能的情况下报告测试集的性能。如果测试标签不公开可用，我们报告开发集的性能。如果数据集的官方分割不存在，则通过选择20％的示例作为测试集和其余示例作为训练集来创建训练和测试分割。所有少量样本上下文示例都从训练集中进行抽样。为了减少少样本评估的方差，我们对每个测试示例进行不同的样本数抽样，除非另有说明。为了保持一致性，对于每个测试示例，在我们的评估中，所有模型具有相同的表面形式作为输入。

#### 5.2 留存损失

我们首先测试BloombergGPT模型在正式金融数据的语言分布上的表现。我们在一个包含FinPile所有部分的留存数据集上评估不同模型的每字节比特数。为了限制数据泄露并更好地模拟LLM的实际使用，我们选择一个严格比训练集更远的时间留存数据集，并在训练集和留存集之间进行去重。在评估过程中，对于长度超过2048个 token的文档，我们使用滑动窗口方法和一半的窗口大小作为上下文。这意味着预测期后的第一个2048个 token以外的任何 token都至少有1024个 token作为上下文。我们根据FinPile文档类型报告损失细分。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%209.png)

图3显示BloombergGPT始终优于其他模型。虽然这是预期的并主要用作健全性检查，但它也提供了其他模型泛化能力的有价值洞察。例如，与BloombergGPT的差距在提交类别中最明显，这可能是因为这些文档虽然是公开的，但通常以PDF格式呈现，因此不包括在任何现有数据集中。

#### 5.3 金融任务

金融领域中最常考虑的NLP任务在更广泛的NLP文献中也很常见；但是，在金融数据上执行这些任务时，它们具有不同的特征和挑战。以情感分析为例，如“公司将裁员1万人”，这样的新闻标题在一般情况下表示负面情感，但有时可能对公司的财务情感是积极的，因为这可能会导致股价或投资者信心提高。我们使用公共和内部基准来评估BloombergGPT、BLOOM176B、GPT-NeoX和OPT66B的性能。表7显示了考虑的所有任务类型及其相应的提示模板。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2010.png)

##### 5.3.1 External Financial Tasks

我们的公共财务基准包括来自FLUE基准（Shah等人，2022年）和ConvFinQA数据集（Chen等人，2022年）的四个任务。由于大多数情况下LLM在这些财务任务上的表现尚未广泛报道，因此没有标准的测试框架。因此，我们将它们适应到少样本的设置中（请参见第5.1节）。我们在设计实验时的指导原则是选择样本数量，使得所有模型的平均表现最佳。虽然这些任务的非LLM定制模型的数量可用，但由于评估设置的差异，我们在此省略了它们的报告。因此，我们的声明仅限于LLM的比较。我们在以下任务上进行评估（有关更多详细信息，请参见附录B）：

- FPB（Malo等人，2014年）：金融短语库数据集包括对来自金融新闻的句子进行情感分类的任务。任何可能有益/有害于投资者的新闻都被认为是积极/消极的，否则是中性的。我们创建自己的拆分，并在5样本设置中报告按支持加权的F1分数。
- FiQA SA（Maia等人，2018年）：第二个情感分析任务是预测英语金融新闻和微博标题中的特定方面的情感，这些新闻是作为2018年金融问题回答和意见挖掘挑战的一部分发布的。尽管原始数据集是在连续尺度上注释的，但我们将数据离散化为具有负面、中性和积极类别的分类设置。与FPB类似，我们创建自己的拆分，包括微博和新闻，并使用5样本设置，报告按权重F1。
- 标题（Sinha和Khandait，2020）：这是一个二元分类任务，用于确定黄金商品领域的新闻标题是否包含特定信息。这个人工注释的数据集包含关于“黄金”的英文新闻标题。每篇新闻文章都带有以下标签的子集：“价格或不价格”，“价格上涨”，“价格下跌”，“价格稳定”，“过去的价格”，“未来的价格”，“过去的一般情况”，“未来的一般情况”，“资产比较”。我们使用官方文档将每个标签口头化成一个问题，使用5个样本，并报告所有类别的平均加权F1分数。
- NER（Salinas Alvarado等人，2015）：这是一个命名实体识别任务，用于从提交给SEC的金融协议中收集的金融数据中进行信用风险评估。注释的实体类型遵循标准CoNLL格式（Tjong Kim Sang和De Meulder，2003），并用PER，LOC，ORG和MISC进行注释。由于在少样本设置中学习预测空输出是非常困难的，因此我们删除不包含任何实体的句子。由于其模糊的定义，我们进一步删除了MISC标签。所有模型都需要更多的样本才能表现良好，因此我们选择了20个样本，并报告了实体级F1分数。
- ConvFinQA（Chen等人，2022）：给定来自S＆P 500盈利报告的输入，其中包括文本和至少一个包含金融数据的表格，任务是回答需要对输入进行数值推理的会话问题。该任务需要数值推理、结构化数据和金融概念的理解，模型需要将后续问题与对话转换相关联。

对于ConvFinQA，我们使用整个黄金对话，并将其上下文用作模型的输入。随着每个对话“轮次”的结束，“轮次”以及该轮次的答案被追加为未来轮次的上下文。我们报告公共开发集上的精确匹配准确性。

BloombergGPT模型在五个任务中的四个（ConvFinQA，FiQA SA，FPB和Headline）表现最佳，在NER中排名第二（表8）。因此，BloombergGPT在我们测试的所有模型中具有最高的胜率。相对于同等规模的模型，它在ConvFinQA方面的差距尤为显著，因为它需要使用对话输入来推理表格并生成答案。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2011.png)

##### 5.3.2 内部任务：情感分析

对于彭博内部任务，我们考虑特定方面的情感分析，在金融文献中很普遍。我们使用的所有数据集都是英文的。

我们的注释过程包括发现阶段，在此阶段中，我们建立注释和抽样过程，了解每个示例通常需要多少注释者，以及注释者需要的培训水平（Tseng等人，2020）。根据任务的复杂性，我们的注释者是彭博的专业金融专家团队、咨询工作者或两者的组合。在每种情况下，通过额外的注释者仲裁并排除模棱两可的示例。本节中的所有数据集都由2个注释者注释，第三个注释者打破任何关系。

我们使用类似于外部数据集的五次评估来衡量LLMs在内部数据集上的性能。由于数据集很大，我们随机抽取最多1k个测试示例。我们报告每个标签的支持度加权的F1。请注意，与外部数据集类似，我们在内部数据集中使用的未 token版本的数据很可能出现在FinPile中，因此BloombergGPT在训练期间也会看到这些数据。但是，由于FinPile的一部分也可在Web上获得，我们将与之比较的其他LLMs也可能在这些数据的未 token版本上进行了训练。数据集统计信息在表9中提供。

- ***股票新闻情感：***此任务是预测新闻故事中表达的对公司的特定方面的情感。数据集由彭博、高级和Web内容的英文新闻故事组成。 “积极”，“消极”或“中性”的注释表明新闻故事可能会增加，减少或不会改变长期投资者对公司的信心。
- ***股票社交媒体情感：***此任务类似于“股权新闻情感”，但我们使用与金融相关的英文社交媒体内容。
- ***股票转录情感：***此任务也类似于“股权新闻情感”，但我们使用公司新闻发布会的转录。转录是通过语音识别和有时是人工编辑来提供的。长转录被分块处理，我们数据集中的每个块通常包含70到80个 token。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2012.png)

- ***ES 新闻情感：***虽然这个任务是预测新闻报道中针对公司（方面）表达的特定情感，但目标并不是表明对投资者信心的影响。如果新闻报道包含反映公司环境和社会政策好、坏或中性新闻的内容，则将其注释为“正面”、“负面”或“中性”。
- ***国家新闻情感：***这个任务与其他情感任务不同，其目标是预测新闻报道中针对一个国家表达的情感。数据集包括来自彭博社、高端和网络内容的英文新闻报道。如果新闻报道暗示该国经济的增长、萎缩或现状，则将其注释为“正面”、“负面”或“中性”。

表10显示，在四个内部特定方面情感任务中，BloombergGPT表现比所有其他测试模型都要好，差距很大。唯一表现相似的任务是社交媒体情感任务，而BloombergGPT在其他三个任务中的表现至少比其他模型高25分，最高可超过60分。

##### 5.3.3 Exploratory Task: NER

尽管NER是一个有着最先进结果的NLP任务，可以使用BERT Wu和Dredze（2019）; Luoma和Pyysalo（2020）和T5 Liu等人（2022）风格的模型，但对于生成式LLM来说，NER主要是一个未被探索的任务。NER不在HELM Liang等人（2022）中，BIG-bench Srivastava等人（2022）中有一个单一的（波兰）任务，我们研究的LLM论文中没有一个报告NER性能。因此，我们认为NER是一个探索性任务，并报告了初步的NER结果，考虑到它在金融领域的重要性。

有几个原因说明为什么NER可能是生成式LLM的一个困难任务。NER是一项信息提取任务，更适合编码器-解码器或仅编码器架构。LLM的生成性质并不为NER带来优势。我们发现，相比其他任务，需要进行广泛的提示工程和更多的shots才能获得合理的NER结果。金融专用NER有一些微妙之处，这使得零或few-shot学习尤其困难。

例如，考虑“彭博社：马斯克先生为Twitter添加新功能并评论中国”的这个我们虚构的标题。根据我们的注释指南和下游任务需求：（a）报告新闻机构“彭博社”可以被 token或不被 token，这取决于我们是否只需要显著实体，（b）需要 token“马斯克先生”或只需 token“马斯克”作为PER，（c）“Twitter”可以被 token为ORG或PRD（产品），因为功能是添加到Twitter产品而不是组织，（d）“中国”可以被 token为ORG或LOC，尽管正确的 token很可能是ORG。如果没有在提示中添加广泛的注释指南，则LLM不知道预期的 token行为。

根据初步测试，我们确定了以下设置，以获得所有模型的内部NER任务的最佳性能。首先，我们限制要预测的实体类型为ORG，PER和LOC。总共，我们过滤掉少于1％的实体。我们还删除所有不包含实体（即所有“O”的）的文档。这两个修改的目的是增加few-shot提示中所看到的示例的有用性。我们预计，进一步进行NER的提示工程工作可以产生更好的结果。

我们考虑了七个彭博内部NER数据集，来自不同的领域：

- BN NER：这是一个命名实体识别任务，用于识别英语长篇彭博新闻内容（“BN wire”）中出现的实体，时间范围为2017年至2020年。
- BFW NER：与“BN NER”类似，但我们使用“Bloomberg First Word”线路中的短篇故事，时间范围为2018年至2020年。•文本NER：该任务的目标是识别公司提交的强制性财务披露中出现的实体。该数据集包含2016年至2019年之间的文件。
- 头条新闻NER：该任务的目标是识别英语彭博新闻内容的标题中出现的实体。该数据集包含2016年至2020年之间的头条新闻。
- 高级NER：该任务的目标是识别彭博摄入的第三方英语新闻内容的子集中出现的实体。该数据集包含2019年至2021年之间的故事。
- Transcripts NER：该任务的目标是识别公司新闻发布会记录中出现的实体。该数据集包含2019年的记录。
- 社交媒体命名NER：此任务的目标是识别出发生在英语金融相关社交媒体内容中的实体。数据集包含在2009年至2020年间采样的社交媒体内容。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2013.png)

由于我们的数据集很大，我们从每个过滤后的内部数据集中随机抽取4,000个训练样本和500个测试样本。我们使用20-shot提示进行评估，使用F1指标进行评估。内部命名实体识别（NER）任务的结果参差不齐（表12）。规模更大的BLOOM176B在大多数NER任务中获胜。在相同规模的模型中，BloombergGPT表现最佳，一次排名第一（Headlines），四次排名第二（BN，Premium，Transcripts，社交媒体），一次排名第三（BFW），一次排名最后（Filings）。

探索性任务：NER+NED命名实体消歧（NED）将实体提及链接到知识库或其他结构化信息源中的已知实体。在金融领域内，我们试图将公司的文本提及与它们的股票代码相链接，股票代码是唯一标识特定股票在特定股票市场上公开交易的缩写。

我们通过评估联合NER+NED任务直接测试LLM完成此任务的能力：识别文档中提到的公司的股票代码。这就要求模型首先识别公司提及，然后生成相应的股票代码。例如，给定“AAPL宣布他们将来的产品中停止使用英特尔芯片。”正确的NER输出应该是“AAPL，Intel”，而正确的NER+NED输出应该是“AAPL，INTC”。

这项任务的优点之一是它对提取确切文本跨度的变化具有强韧性。虽然NER评估需要精确匹配，但可以成功生成股票代码，而无需首先识别跨度。此外，它还评估了模型对公司、它们的各种表面形式和公司到股票代码映射的了解。

我们通过对每个领域的Bloomberg内部NER注释文档运行金融数据中公司的最先进的实体链接系统来创建此任务的链接股票的评估数据。我们删除没有链接股票的文档。在我们的NER评估之后，我们从每个过滤后的内部数据集中随机抽取4,000个训练样本和500个测试样本。我们使用20-shot提示进行评估，使用F1指标进行评估。表12显示，除了社交媒体数据，BloombergGPT在其他所有模型中表现都远远优于其他模型。在我们的社交媒体数据中，公司通常以它们的股票代码被提及，从而消除了模型链接提及和将任务转换为NER的要求。这些结果进一步强调了BloombergGPT在金融任务中的优势。

#### 5.4 BIG-bench Hard

现在我们转向评估BloombergGPT在标准的通用自然语言处理任务上的表现。虽然我们的模型的重点是金融任务，但我们包含通用训练数据可能不仅有助于改善金融任务，还可以使我们的模型在更标准的NLP数据集上表现良好。我们从BIG-bench Hard（Suzgun等人，2022）开始，这是BIG-bench（Srivastava等人，2022）中最具挑战性的任务的子集。它仅包括最好的现有模型无法通过标准提示技术实现高于平均人类评分者的性能的任务。

每个任务的结果如表13所示。总体而言，虽然BloombergGPT落后于规模大得多的PaLM540B（参数量的10倍）和BLOOM176B（参数量的3.5倍），但它是规模相似的模型中表现最好的。实际上，它的性能更接近BLOOM176B，而不是GPT-NeoX或OPT66B。它进一步在日期理解、逆序（形容词的顺序）和跟踪洗牌对象方面实现了所有模型的最佳性能。总之，根据此基准测试，我们发现开发金融特定的BloombergGPT并没有损害其通用能力。

#### 5.5 知识评估

接下来，我们通过模型回答问题而不提供额外的上下文或资源（闭卷问答）的情景来评估知识，我们将知识定义为回忆在模型训练期间看到的信息的能力。这包括多项选择问题，并报告准确率。我们遵循Brown等人的模板（2020）。情景列表如下：

- ARC（Clark等人，2018）：从第3至9年级的科学考试中收集的多项选择问题，包括易于和具有挑战性的分割。
- CommonsenseQA（Talmor等人，2019）：多项选择问答（QA）数据集，需要不同类型的常识知识。
- MMLU（Hendrycks等人，2021）：57个学科中手动收集的多项选择知识问题。
- PhysicalQA（PiQA，Bisk等人，2020）：有关物理世界运作方式的问题。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2014.png)

BloombergGPT在一个任务中表现最佳，BLOOM176B、GPT-NeoX和OPT66B在另外三个任务中排名第二（表14）。与前面的部分类似，它在几乎与更大的模型相当的情况下，优于相似大小的模型。 Massive Multitask Language Understanding（MMLU，Hendrycks等人，2021）涵盖57个不同的学科，因此比上述任务具有更广泛的覆盖范围。表15中的聚合结果描绘了更一致的图片，并遵循BIG-bench hard中看到的见解。BloombergGPT始终优于OPT66B，OPT66B又优于GPT-NeoX，而GPT-3表现最佳。

与前面的部分相比，BloombergGPT在这个类别中也表现优异，尽管差距较小，但仍优于BLOOM176B。它在社会科学类别中的表现落后于GPT-3，尤其是在社会科学类别中。与GPT-3的差距在STEM和“其他”领域最接近，这些领域包括与金融和会计相关的问题。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2015.png)

#### 5.6 阅读理解

我们将阅读理解基准定义为模型可以根据呈现的输入文本中包含的信息生成正确响应的任务。我们的分组包括开放书籍问答任务，而不是Brown等人（2020）将它们分为不同的类别。我们遵循Brown等人（2020）的模板，并报告准确率。我们包括以下任务

- BoolQ（Clark等人，2019）：关于维基百科文章的是/否问题。
- OpenBookQA（Mihaylov等人，2018）：多项选择的初中科学问题，给出科学事实书，应用于新情况。
- RACE（Lai等人，2017）：中学和高中英语考试的多项选择数据集。
- 多句子阅读理解（MultiRC，Khashabi等人，2018）：简短段落和多句子问题。
- 常识推理阅读理解（ReCoRD，Zhang等人，2018）：有关CNN和每日邮报新闻文章的自动生成问题。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2016.png)

表16反映了与上述评估类似的排名：虽然GPT-3的性能最高，但BloombergGPT是一个紧随其后的第二名。除了OpenBookQA外，BloombergGPT在BLOOM176B、GPT-NeoX和OPT66B中的表现最好。令人惊讶的是，BLOOM176B在这个类别中明显落后。

#### 5.7 语言任务

我们将与用户界面应用程序不直接相关的情景定义为语言任务。这些任务包括评估消歧、语法或蕴含的任务。这些任务旨在直接评估模型理解语言的能力。我们遵循Brown等人（2020）的模板，并报告准确率。任务列表如下：

- ***文本蕴涵识别***（RTE，Dagan等人，2007；Haim等人，2006；Giampiccolo等人，2007；Bentivogli等人，2009）：给定两个文本片段，确定一个文本的意义是否意味着另一个文本的意义。
- ***对抗NLI***（ANLI，Nie等人，2020）：对抗构造的蕴涵检测。
- ***CommitmentBank***（CB，De Marneffe等人，2019）：自然发生的话语，其最后一句包含一个从句嵌入谓词。
- ***选择合适的替代方案***（COPA，Gordon等人，2011）：前提和两个替代方案，任务是选择与前提更有因果关系的替代方案。
- ***上下文中的单词***（WIC Pilehvar和Camacho-Collados，2019）：确定一个单词是否在两个句子中使用相同的含义。
- Winograd（Levesque等，2011年）：确定代词指的是哪个词，当它在语义上不含糊。
- Winogrande（Sakaguchi等，2019年）：对抗性挖掘具有挑战性的Winograd例子。
- HellaSWAG（Zellers等，2019年）：选择故事或一组指令的最佳结局。
- StoryCloze（Mostafazadeh等，2016年）：为长达五个句子的故事选择正确的结尾句子。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2017.png)

语言任务结果（表17）遵循与知识类别类似的趋势。BloombergGPT略微落后于GPT-3并且优于其他模型。类似于阅读理解类别，BLOOM176B落后于BloombergGPT。

#### 5.8 总结

在多个基准测试中，通过数十个任务的比较，可以得出一个清晰的结论。在我们比较的数十亿参数的模型中，BloombergGPT表现最佳。此外，在某些情况下，它具有竞争力，甚至超过了更大模型（数千亿的参数）。虽然我们的目标是让BloombergGPT成为金融任务的最佳模型，并包含通用域数据以支持特定领域的训练，但该模型仍然在通用数据上获得了超越类似大小模型的能力，在某些情况下，甚至与更大的模型相匹配或超过。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2018.png)

### 6. 高质量样例

我们现在分享我们的模型的定性示例，突出了我们领域专业化模型的益处。

***生成彭博查询语言***。BloombergGPT的一个用例是使与金融数据的交互更加自然。获取数据的现有方法是通过彭博查询语言（BQL）。BQL可用于与不同类别的证券交互，每个证券都有自己的字段、函数和参数。BQL是一个非常强大但复杂的工具。如图4所示，可以利用BloombergGPT将自然语言查询转换为有效的BQL，使BQL更加易于访问。

***新闻标题建议***。另一个得到很好支持的用例是在新闻领域。由于它是在许多新闻文章上进行训练的，因此可以用于许多新闻应用程序，并协助记者进行日常工作。例如，在构建通讯时，记者可能必须为每个新部分编写简短的标题。虽然专门的模型可帮助完成此任务可能成本太高，但BloombergGPT在开箱即用时表现良好（图5）。

***金融问答***。由于金融领域的训练数据，我们能够查询BloombergGPT以获取与金融世界相关的知识。例如，它在识别公司CEO方面表现良好。图6显示了包括其他模型的输出在内的几个示例。虽然BloombergGPT正确地识别了CEO，但GPT-NeoX没有，而FLAN-T5-XXL则完全失败，一直忽略公司，而是预测包含在提示中的Cirrus Logic的CEO。虽然BloombergGPT不能完美地解决这个任务并且会犯错误，但我们找不到其他模型解决这个任务而BloombergGPT没有的例子。

![Untitled](../../assets/6.1麻瓜读论文/BloombergGPT/Untitled%2019.png)

### 9 总结

我们展示了BloombergGPT，这是一款用于金融自然语言处理的最佳LLM。我们的模型对于训练特定领域模型的有效方式做出了贡献。我们的训练策略将特定领域和通用数据混合，产生了一个在两个领域性能均衡的模型。此外，我们的工作提供了选择Chinchilla最优大小模型的另一个数据点。最后，我们希望我们的模型训练日志能够为那些正在训练自己的LLMs的人提供指导。

我们有几个有趣的方向需要追求。首先，任务微调在LLMs中产生了显著的改进，我们计划考虑在金融领域中进行模型对齐的独特机会（Wei等，2021；Ouyang等，2022）。其次，通过在FinPile中训练数据，我们选择可能呈现出较少有害和偏见语言的数据。这对最终模型的影响尚不清楚，我们计划进行测试。第三，我们希望了解我们的分词策略如何改变生成的模型。这些是我们希望通过BloombergGPT追求的一些新的研究方向。

我们在通用LLM基准测试中取得了强大的结果，并在金融任务上优于可比较的模型。我们归因于以下三个因素，按影响减少的顺序：1. 精选的内部数据集，2. 我们独特的分词器选择，3. 最新的架构。我们将继续使用BloombergGPT开发金融应用程序，以进一步探索这些建模选择的好处。